网卡接受消息流程（linux6.10）

1. module_init(igb_init_module);

2. static int __init igb_init_module(void)
	注册pci：pci_register_driver(&igb_driver);

3. igb_probe				
	（1）配置igb_netdev_ops：netdev->netdev_ops = &igb_netdev_ops; igb_set_ethtool_ops(netdev);
	（2）在最后调用igb_sw_init，再进入igb_init_interrupt_scheme(adapter, true)来初始化中断方案
		我们层层进入igb_alloc_q_vectors，igb_alloc_q_vector，在netif_napi_add(adapter->netdev, &q_vector->napi, igb_poll);中注册igb_poll用来进行napi的poll轮询
	在net_dev_init方法中，注册了软中断：
	open_softirq(NET_TX_SOFTIRQ, net_tx_action);
	open_softirq(NET_RX_SOFTIRQ, net_rx_action);

4. register_netdev
	在igb_probe中调用register_netdev注册网络设备

5. igb_netdev_ops中注册的.ndo_open函数=igb_open

6. igb_open
	igb_open中调用err = igb_setup_all_rx_resources(adapter);在这里再调用igb_setup_rx_resources(adapter->rx_ring[i])分配接收描述符数组。
	igb_open中调用err = igb_setup_all_tx_resources(adapter);在这里再调用igb_setup_tx_resources(adapter->tx_ring[i])分配发送描述符数组。
	igb_open中调用err = igb_request_irq(adapter);这里配置中断回调函数
	调用request_irq(pdev->irq, igb_intr, IRQF_SHARED, netdev->name, adapter);会在igb_intr中napi_schedule(&q_vector->napi);触发软中断事件

7. igb_setup_rx_resources和igb_setup_tx_resources
	分配RX缓冲区(rx_buffer_info)：struct igb_rx_buffer
	计算 desc 所需的总字节数，并 按 4KB 对齐，保证 DMA 访问高效（rx_ring->desc = dma_alloc_coherent(dev, rx_ring->size, &rx_ring->dma, GFP_KERNEL);）
	union e1000_adv_rx_desc：这个数组是网卡硬件使用的，硬件是可以通过 DMA 直接访问这块内存，通过 dma_alloc_coherent 分配
	
	分配TX缓冲区(tx_buffer_info)：struct igb_tx_buffer
	计算 desc 所需的总字节数，并 按 4KB 对齐，保证 DMA 访问高效（tx_ring->desc = dma_alloc_coherent(dev, tx_ring->size, &tx_ring->dma, GFP_KERNEL);）
	union e1000_adv_tx_desc：这个数组是网卡硬件使用的，硬件是可以通过 DMA 直接访问这块内存，通过 dma_alloc_coherent 分配

8. igb_request_irq
	（1）如果flag=IGB_FLAG_HAS_MSIX，调用igb_request_msix
		err = request_irq(adapter->msix_entries[vector].vector, igb_msix_ring, 0, q_vector->name, q_vector);
		这里调用igb_msix_ring，在该函数中调用napi_schedule(&q_vector->napi);
		注意：在____napi_schedule中硬件中断最终触发的软中断是NET_RX_SOFTIRQ
		if (!sd->in_net_rx_action)
			__raise_softirq_irqoff(NET_RX_SOFTIRQ);			//在中断关闭（irqoff）状态下触发软中断
		这就是为什么查看/proc/softirqs，为什么NET_RX要比NET_TX多得多的原因
	（2）正常流程调用igb_intr()方法
		napi_schedule(&q_vector->napi);
		这会将 q_vector->napi 加入 poll_list，并触发 NET_RX_SOFTIRQ 软中断。
		关键：此时禁用RX中断，防止中断风暴。

9. igb_poll
	（1）如果q_vector->rx.ring中的接受队列有值，调用int cleaned = igb_clean_rx_irq(q_vector, budget);

10. igb_clean_rx_irq
	napi_gro_receive(&q_vector->napi, skb);
	
11. napi_gro_receive
	调用ret = napi_skb_finish(napi, skb, dev_gro_receive(napi, skb));

12. napi_skb_finish
	调用gro_normal_one(napi, skb, 1);

13. gro_normal_one
	调用gro_normal_list(napi);

14. gro_normal_list
	调用netif_receive_skb_list_internal(&napi->rx_list);

15. netif_receive_skb_list_internal
	调用__netif_receive_skb_list
	调用__netif_receive_skb_list_core(&sublist, pfmemalloc);

16. __netif_receive_skb_list_core
	调用__netif_receive_skb_list_ptype(&sublist, pt_curr, od_curr);

17. __netif_receive_skb_list_ptype：判断ipv4或者ipv6处理
	INDIRECT_CALL_INET(pt_prev->list_func, ipv6_list_rcv,
				   ip_list_rcv, head, pt_prev, orig_dev);
	这里以ip_list_rcv为例子，继续调用ip_sublist_rcv

18. ip_list_rcv_finish
	调用ip_sublist_rcv_finish(&sublist);

19. ip_sublist_rcv_finish
	调用dst_input(skb);

20. dst_input
	static inline int dst_input(struct sk_buff *skb)
	{
		return INDIRECT_CALL_INET(skb_dst(skb)->input,
				  ip6_input, ip_local_deliver, skb);
	}
	调用ip_local_deliver

21. ip_local_deliver
	return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN,
		       net, NULL, skb, skb->dev, NULL,
		       ip_local_deliver_finish);
	调用ip_local_deliver_finish

22. ip_local_deliver_finish
	调用ip_protocol_deliver_rcu(net, skb, ip_hdr(skb)->protocol);

23. ip_protocol_deliver_rcu
	调用ret = INDIRECT_CALL_2(ipprot->handler, tcp_v4_rcv, udp_rcv, skb);

24. tcp_v4_rcv
	调用ret = tcp_v4_do_rcv(sk, skb);

25. tcp_v4_do_rcv
	调用tcp_rcv_established(sk, skb);

26. tcp_rcv_established
	调用tcp_data_queue(sk, skb);

27. tcp_data_queue
	调用eaten = tcp_queue_rcv(sk, skb, &fragstolen);

28. tcp_queue_rcv：核心函数
	static int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,
				      bool *fragstolen) {
		int eaten;
		struct sk_buff *tail = skb_peek_tail(&sk->sk_receive_queue);

		eaten = (tail &&
			 tcp_try_coalesce(sk, tail,
					  skb, fragstolen)) ? 1 : 0;
		tcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)->end_seq);
		if (!eaten) {
			__skb_queue_tail(&sk->sk_receive_queue, skb);
			skb_set_owner_r(skb, sk);
		}
		return eaten;
	}
	最终，把skb放到sk->sk_receive_queue队列中，等待用户读取


用户进程读取网卡数据
1. recv
	SYSCALL_DEFINE4(recv, int, fd, void __user *, ubuf, size_t, size,
		unsigned int, flags)
	{
		return __sys_recvfrom(fd, ubuf, size, flags, NULL, NULL);
	}

2. __sys_recvfrom
	调用：err = sock_recvmsg(sock, &msg, flags);

3. sock_recvmsg
	调用sock_recvmsg_nosec

4. sock_recvmsg_nosec
	int ret = INDIRECT_CALL_INET(sock->ops->recvmsg, inet6_recvmsg,
			     inet_recvmsg, sock, msg,
			     msg_data_left(msg), flags);
	调用inet_recvmsg

5. inet_recvmsg
	err = INDIRECT_CALL_2(sk->sk_prot->recvmsg, tcp_recvmsg, udp_recvmsg,
			      sk, msg, size, flags, &addr_len);
	调用tcp_recvmsg

6. tcp_recvmsg
	调用ret = tcp_recvmsg_locked(sk, msg, len, flags, &tss, &cmsg_flags);

7. tcp_recvmsg_locked
	如果是URGENT（带外数据MSG_OOB），则调用tcp_recv_urg()，这个里面很明显看到只复制一个字符
	调用skb_copy_datagram_msg进入正常流程将 sk_buff 数据拷贝到用户空间缓冲区中。

8. skb_copy_datagram_msg
	调用skb_copy_datagram_iter

9. skb_copy_datagram_iter
	调用skb_copy_datagram_iter

-------------------------------------------------------------------------------------------------------------------------------------------------
igb网卡发送数据流程

用户进程发送数据到网卡流程
1. SYSCALL_DEFINE4(send)
	调用__sys_sendto

2. __sys_sendto
	调用err = sock_sendmsg(sock, &msg);

3. sock_sendmsg
	调用return err ?: sock_sendmsg_nosec(sock, msg);

4. sock_sendmsg_nosec
	int ret = INDIRECT_CALL_INET(sock->ops->sendmsg, inet6_sendmsg, inet_sendmsg, sock, msg, msg_data_left(msg));
	调用inet_sendmsg
			
5. inet_sendmsg
	return INDIRECT_CALL_2(sk->sk_prot->sendmsg, tcp_sendmsg, udp_sendmsg, sk, msg, size);
	这里以tcp为例，调用tcp_sendmsg

6. tcp_sendmsg
	调用tcp_sendmsg_locked

7. tcp_sendmsg_locked
	调用tcp_skb_entail(sk, skb);

8. tcp_skb_entail
	调用tcp_add_write_queue_tail(sk, skb);

9. tcp_add_write_queue_tail
	把skb塞到sk->sk_write_queue队列里面，等待网卡进行读取发送

第二部分，用户空间发送数据到网络协议栈
10. tcp_sendmsg_locked
	调用tcp_push(sk, flags, mss_now, tp->nonagle, size_goal);

11. tcp_push
	调用__tcp_push_pending_frames(sk, mss_now, nonagle);

12， __tcp_push_pending_frames
	调用tcp_write_xmit(sk, cur_mss, nonagle, 0, sk_gfp_mask(sk, GFP_ATOMIC))

13. tcp_write_xmit
	调用tcp_transmit_skb(sk, skb, 1, gfp)

14. tcp_transmit_skb
	调用__tcp_transmit_skb(sk, skb, clone_it, gfp_mask, tcp_sk(sk)->rcv_nxt)

15. __tcp_transmit_skb
	err = INDIRECT_CALL_INET(icsk->icsk_af_ops->queue_xmit, inet6_csk_xmit, ip_queue_xmit, sk, skb, &inet->cork.fl);
	这个以ipv4为例子，调用ip_queue_xmit

16. ip_queue_xmit
	调用__ip_queue_xmit

17. __ip_queue_xmit
	调用res = ip_local_out(net, sk, skb);

18. ip_local_out
	err = __ip_local_out(net, sk, skb);

19. __ip_local_out
	return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT, net, sk, skb, NULL, skb_dst(skb)->dev, dst_output);
	这里调用dst_output

20. dst_output
	return INDIRECT_CALL_INET(skb_dst(skb)->output, ip6_output, ip_output, net, sk, skb);
	这里以ipv4为例子，调用ip_output

21. ip_output
	return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, net, sk, skb, indev, dev, ip_finish_output, !(IPCB(skb)->flags & IPSKB_REROUTED));
	这里调用ip_finish_output来发送实际的数据包

22. ip_finish_output
	调用__ip_finish_output

23. __ip_finish_output
	调用ip_finish_output2(net, sk, skb);

24. ip_finish_output2
	res = neigh_output(neigh, skb, is_v6gw);根据目标路由查找相应的邻居（下一跳），然后将数据包发送给邻居。如果邻居找不到，丢弃数据包并返回错误。

25. neigh_output
	if (!skip_cache &&
	    (READ_ONCE(n->nud_state) & NUD_CONNECTED) &&
	    READ_ONCE(hh->hh_len))
		return neigh_hh_output(hh, skb);

	return n->output(n, skb);
	(1) 如果 skip_cache 为 false，且目标邻居的状态是 NUD_CONNECTED（即目标设备已连接，邻居缓存是有效的），并且邻居的硬件头长度 hh_len 非零，neigh_output 会通过 neigh_hh_output 将数据包直接发送到链路层设备。这通常是通过已有的硬件缓存（如 MAC 地址）来进行发送。
	(2) 如果不使用缓存或没有缓存可用，neigh_output 会调用邻居的 output 函数来发送数据包。这个 output 函数通常是由网络设备的驱动程序提供的，用于实际将数据包传送到链路层。
	
	static const struct neigh_ops arp_generic_ops = {
		.family =		AF_INET,
		.solicit =		arp_solicit,
		.error_report =		arp_error_report,
		.output =		neigh_resolve_output,
		.connected_output =	neigh_connected_output,
	};
	这个钩子在/net/ipv4/af_inet.c中fs_initcall(inet_init)初始化，再调用arp_init()，最后调用neigh_table_init(NEIGH_ARP_TABLE, &arp_tbl)，初始化neighbour；在arp_tbl.constructor中调用arp_constructor方法来初始化arp，并执行neigh->ops = &arp_direct_ops，把钩子完全挂上

	这里output指向的是neigh_resolve_output函数，进而调用__dev_queue_xmit，再调用dev_hard_start_xmit，再xmit_one，再netdev_start_xmit，见下函数：

	static inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,
					      struct sk_buff *skb, struct net_device *dev,
					      bool more)
	{
		__this_cpu_write(softnet_data.xmit.more, more);
		return ops->ndo_start_xmit(skb, dev);
	}

	static inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,
					    struct netdev_queue *txq, bool more)
	{
		const struct net_device_ops *ops = dev->netdev_ops;
		netdev_tx_t rc;

		rc = __netdev_start_xmit(ops, skb, dev, more);
		if (rc == NETDEV_TX_OK)
			txq_trans_update(txq);

		return rc;
	}
	最终在__netdev_start_xmit函数中，调用ops->ndo_start_xmit(skb, dev)，而在igb_main.c中，我们已经绑定了ndo_start_xmit指向igb_xmit_frame。
	所以如果是igb网卡，这里output走的就是igb_xmit_frame函数

26. igb_xmit_frame
	调用igb_xmit_frame_ring()
	igb_tx_map(tx_ring, first, hdr_len)函数负责将数据包（包括头部和数据段）映射到 DMA 地址，并为发送队列中的每个描述符（用于网络数据的传输）分配缓冲区。
	
	最终调用writel(i, tx_ring->tail);来告知硬件网卡(向硬件寄存器写入更新后的描述符指针)，有数据需要发送

27. igb_clean_tx_irq
	由于在igb_poll中声明了发送队列的处理：
	if (q_vector->tx.ring)
		clean_complete = igb_clean_tx_irq(q_vector, budget);
	这里在网卡发送完成后，会处理已经完成传输的包，并回收相关资源。